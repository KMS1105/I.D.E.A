[[목차]]

슬라이드1

2학기 동아리 발표를 시작하겠습니다.


슬라이드2

제가 준비한 내용은 제가 만든 감정 분석 AI에 대한 것 입니다.


슬라이드3

목차입니다.
개발 목적,
전 처리,
메인 AI 개발,
느낀 점 순으로 발표하겠습니다.


슬라이드4

제가 이 AI를 만들려고 한 이유는
1학기에 발표한 내용을 토대로 최대한 탐구한 내용을 사용하여 AI를 만들어보자는 생각을 하던중에 이러한 조건에 부합하는, 너무 개발 기간이 오래 걸리지도 않는, 그렇다고 너무 단순하지도 않은 그런 AI인 감정 분석 AI를 찾았고, 기본적인 AI들을 바탕으로 정확도 향상을 위해 기존 데이터 셋보다 더 많은 데이터가 들어있는 새로운 데이터 셋으로 새롭게 전 처리 과정과 메인 AI를 만들었습니다. 

(해당 AI는 딥러닝이라기보단 머신러닝에 가깝고 문장 [[임베딩]]을 위해서 [[Sentence Bert]]와 유사도 측정을 위해서 [[코사인 유사도]]를 사용했습니다.)

슬라이드5

데이터를 분석 및 처리에 적합한 형태로 만드는 과정을 총칭합니다.
데이터 전처리는 데이터 분석 및 처리 과정에서 중요한 단계이고 데이터 분석,
데이터 마이닝, 머신 러닝 프로젝트에 적용 합니다. 그리고 일반적으로 데이터는 비어있는 부분이 있거나 정합성이 맞지 않는 경우가 많이 있습니다.
아무리 좋은 도구나 분석 기법도 품질이 낮은 데이터로는 좋은 결과를 얻기가 힘듭니다.

데이터 전 처리 단계는 보통
데이터 수집 – > 데이터 정제 -> 데이터 통합 -> 데이터 축소 -> 데이터 변환의 가정을 거치게 됩니다.
몇 가지 전 처리 예시로
중복 값 제거, 결측 값 보정, 데이터 연계/통합 , 노이즈 제거, 데이터 구조 변경(차원 변경), 데이터 벡터 화, outlier detection, feature Engineering 등이 있습니다.


슬라이드6

데이터 프레임(Data Frame)은 표 형태의 데이터 구조를 나타내는 데이터 구조입니다. 
주로 데이터 분석과 조작을 위해 사용되며, 표 형태로 정리된 데이터를 컬럼(열)과 로우(행)로 구성합니다.

테이블 형식: 데이터 프레임은 표 형태로 데이터를 저장하며, 각 열은 변수를 나타내고 각 행은 개별 데이터 관측 치를 나타냅니다.

열과 행: 데이터 프레임은 여러 열로 구성되며 각 열은 변수를 나타내며, 각 행은 관측 치를 나타냅니다. 각 행과 열에는 고유한 이름 또는 인덱스가 부여됩니다.


슬라이드7

1. 데이터 셋에서 발화 데이터와 응답 데이터만 필요하고, 응답 데이터에 결측 값이 있어서 이를 채우려고 합니다.
2. 응답 데이터의 결측 값을 유사한 감정에 해당하는 다른 응답 데이터로 채우기 위한 전 처리를 수행하였습니다.
3. 전처리 단계를 구체적으로 설명하면:
    - 테이블 데이터를 가져와 필요 없는 열을 제거하였습니다.
    - 0번째 행과 마지막 행, 결측 값에 0을 채워 넣었습니다.
    - 연속된 행 중에서 첫 번째 행과 마지막 행을 판단하고 그 사이의 행 개수를 계산하였습니다.
    - 묶음 행에 0이 있는 경우 다음 묶음 행 전까지 반복해서 묶음 행을 추가하여 새로운 CSV 파일을 생성하였습니다.

이러한 과정을 통해 응답 데이터의 결측 값을 유사한 감정을 가진 다른 응답 데이터로 채웠습니다.


슬라이드8

[[BERT]]의 문장 [[임베딩]]을 얻는 세 가지 방법은 다음과 같습니다:

[[CLS토큰]]의 출력 벡터를 문장 벡터로 사용하여 BERT의 [[CLS토큰]]을 문장의 총체적 표현으로 간주하는 방법, 평균 풀링, 맥스 풀링입니다.

CLS토큰은 BERT가 분류 문제를 풀기 위해 추가된 특별 토큰입니다. 

평균 풀링(mean pooling): BERT의 모든 단어 출력 벡터들을 평균 내어 문장 벡터로 사용하는 방법이고,
이 방법은 모든 단어의 의미를 반영하는 경향이 있습니다.

맥스 풀링(max pooling): BERT의 모든 단어 출력 벡터들 중 가장 큰 값을 선택하여 문장 벡터로 사용이고, 
이 방법은 중요한 단어의 의미를 강조하는 경향이 있습니다.


슬라이드9

Sentence Bert는 BERT의 문장 임베딩을 향상시킨 모델입니다.

임베딩이란 위와 같이 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾼 결과 혹은 그 과정 전체를 의미합니다.
이렇게 변환시킨 벡터 값에 의미와 정보를 손실없이 잘 담아낼수록 좋은 임베딩이라 할 수 있습니다.

SBERT 모델 파인 튜닝에는 두 가지 주요 방법이 있습니다:

1. **문장 쌍 분류 태스크**: 두 문장 간의 관계를 예측하는데 활용됩니다. 주로 NLI(Natural Language Inferencing) 문제로 사용되며, "수반," "모순," "중립"과 같은 레이블을 예측합니다. 이를 위해 두 문장을 BERT 모델에 입력하여 임베딩 벡터를 얻고, 차이 벡터를 계산한 후 이를 연결하여 다중 클래스 분류 문제로 다룹니다.
    
2. **문장 쌍 회귀 태스크**: 두 문장 간의 의미적 유사도를 예측하는데 사용됩니다. 주로 STS(Semantic Textual Similarity) 문제로 사용되며, 두 문장의 임베딩 벡터를 계산한 후 코사인 유사도를 활용하여 실제 레이블과의 차이를 평균 제곱 오차(MSE)로 최소화합니다. 이렇게 예측된 유사도는 0에서 5 사이의 값으로 조정됩니다.
    

이러한 파인 튜닝 방법을 통해 SBERT 모델은 문장 간의 관계 파악과 의미적 유사성 평가와 같은 다양한 자연어 처리 작업에 활용됩니다.

제가 만든 감정 분석 AI에는 문장 쌍 회귀 태스크 방식이 쓰였습니다.


슬라이드10

기본적인 감정 분석 AI틀과 비슷하게 코드를 짰습니다.
Sentence Bert를 사용하기 위해서 SentenceTransformer모델을 불러오고, 전 처리된 파일을 불러온 뒤 'utterance(2차)'(발화 데이터)를 SentenceTransformer로 인코딩하여 'embedding' 이라는 새로운 칼럼에 넣습니다. 
이후 측정할 인 풋 데이터 역시 모델로 인코딩하고, 'similarity' 칼럼을 추가하여 코사인 유사도를 통해 유사도를 측정합니다. 
'embedding'의 발화 데이터와 인 풋 데이터의 유사도 값이 가장 높은 발화 데이터가 들어있는 행을 출력합니다.


슬라이드11

1.결측 값을 0으로 처리하지 않고 다른 방법을 찾으려 했지만 오류로 어려움을 겪었으며, 현재의 방법을 사용하게 된 점이 아쉬웠습니다.

2.AI 개발은 클론 코딩한 부분이 많이 있어서 비교적 쉽게 진행되었지만, 처음 다뤄본 전 처리 작업은 어려웠고, 시간이 많이 걸렸지만 그래도 동시에 즐거웠던 것 같습니다.


슬라이드12

이상 발표를 마치겠습니다.
제 발표를 들어주셔서 감사합니다.



