[[목차]]
[[목적]]

[[코사인 유사도]]

https://wikidocs.net/156176

[[BERT]]의 문장 [[임베딩]]을 얻는 세 가지 방법은 다음과 같습니다:

[[CLS토큰]]의 출력 벡터를 문장 벡터로 사용: BERT의 [[CLS토큰]]을 문장의 총체적 표현으로 간주하는 방법.

평균 풀링(mean pooling): BERT의 모든 단어 출력 벡터들을 평균 내어 문장 벡터로 사용. 
이 방법은 모든 단어의 의미를 반영하는 경향이 있습니다.

맥스 풀링(max pooling): BERT의 모든 단어 출력 벡터들 중 가장 큰 값을 선택하여 문장 벡터로 사용. 
이 방법은 중요한 단어의 의미를 강조하는 경향이 있습니다.

SBERT(센텐스버트)는 BERT의 문장 [[임베딩]]을 향상시킨 모델로, 
주로 두 가지 방법을 사용하여 파인 튜닝됩니다:

문장 쌍 분류 태스크로 파인 튜닝:
이 방법은 SBERT를 학습하기 위해 문장 쌍 분류 문제를 활용합니다.
대표적인 예로 NLI(Natural Language Inferencing) 문제를 사용하는데, 
이 문제에서 두 문장 간의 관계를 파악하며 다음과 같은 레이블을 예측합니다: 
수반(entailment), 모순(contradiction), 중립(neutral).

구체적으로, 두 문장인 A와 B를 BERT 모델에 입력하여 각각의 문장 임베딩 벡터 u와 v를 얻습니다.
그런 다음, u와 v 간의 차이 벡터 |u-v|를 계산하고, 이 차이 벡터를 포함한 세 가지 벡터를 연결(concatenate)합니다.
연결된 벡터를 출력층에 전달하여 다중 클래스 분류 문제를 푸는데, 분류하고자 하는 클래스 수에 따라 출력층의 크기를 설정하고 소프트맥스 함수를 적용합니다.

문장 쌍 회귀 태스크로 파인 튜닝:
이 방법은 SBERT를 학습하기 위해 문장 쌍 회귀 문제를 활용합니다.
대표적인 예로 STS(Semantic Textual Similarity) 문제를 사용하는데, 이 문제에서 두 문장 간의 의미적 유사도를 예측합니다.
구체적으로, 두 문장인 A와 B를 BERT 모델에 입력하여 각각의 문장 임베딩 벡터를 얻습니다.
그런 다음, 평균 풀링 또는 맥스 풀링을 통해 각각의 벡터 u와 v를 얻습니다.
이후, u와 v 간의 코사인 유사도를 계산하고, 이 유사도와 실제 레이블 유사도와의 평균 제곱 오차(MSE)를 최소화하여 모델을 학습합니다.
코사인 유사도의 값 범위가 -1부터 1까지이므로, 레이블 스코어의 범위가 0에서 5 사이라면 이를 5로 나누어 값의 범위를 조정합니다.
두 방법 중 하나를 선택하여 SBERT 모델을 학습할 수 있으며, 또한 두 방법을 연이어 학습하는 전략도 적용할 수 있습니다. 이러한 방법을 통해 SBERT는 문장 간의 관계 파악 및 의미적 유사성 평가와 같은 다양한 자연어 처리 작업에 활용됩니다.




